{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype = np.float32)\n",
    "y_data = np.array([[0], [1], [1], [0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 2])\n",
    "Y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2, 5]), name = 'weight1')\n",
    "b1 = tf.Variable(tf.random_normal([5]), name = 'bias1')\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([5, 5]), name = 'weight2')\n",
    "b2 = tf.Variable(tf.random_normal([5]), name = 'bias2')\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([5, 5]), name = 'weight3')\n",
    "b3 = tf.Variable(tf.random_normal([5]), name = 'bias3')\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([5, 5]), name = 'weight4')\n",
    "b4 = tf.Variable(tf.random_normal([5]), name = 'bias4')\n",
    "\n",
    "W5 = tf.Variable(tf.random_normal([5, 5]), name = 'weight5')\n",
    "b5 = tf.Variable(tf.random_normal([5]), name = 'bias5')\n",
    "\n",
    "W6 = tf.Variable(tf.random_normal([5, 5]), name = 'weight6')\n",
    "b6 = tf.Variable(tf.random_normal([5]), name = 'bias6')\n",
    "\n",
    "W7 = tf.Variable(tf.random_normal([5, 5]), name = 'weight7')\n",
    "b7 = tf.Variable(tf.random_normal([5]), name = 'bias7')\n",
    "\n",
    "W8 = tf.Variable(tf.random_normal([5, 5]), name = 'weight8')\n",
    "b8 = tf.Variable(tf.random_normal([5]), name = 'bias8')\n",
    "\n",
    "W9 = tf.Variable(tf.random_normal([5, 5]), name = 'weight9')\n",
    "b9 = tf.Variable(tf.random_normal([5]), name = 'bias9')\n",
    "\n",
    "W10 = tf.Variable(tf.random_normal([5, 1]), name = 'weight10')\n",
    "b10 = tf.Variable(tf.random_normal([1]), name = 'bias10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesis():\n",
    "    layer1 = tf.nn.relu(tf.matmul(x_data, W1) + b1)\n",
    "    layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "    layer3 = tf.nn.relu(tf.matmul(layer2, W3) + b3)\n",
    "    layer4 = tf.nn.relu(tf.matmul(layer3, W4) + b4)\n",
    "    layer5 = tf.nn.relu(tf.matmul(layer4, W5) + b5)\n",
    "    layer6 = tf.nn.relu(tf.matmul(layer5, W6) + b6)\n",
    "    layer7 = tf.nn.relu(tf.matmul(layer6, W7) + b7)\n",
    "    layer8 = tf.nn.relu(tf.matmul(layer7, W8) + b8)\n",
    "    layer9 = tf.nn.relu(tf.matmul(layer8, W9) + b9)\n",
    "    cost = tf.sigmoid(tf.matmul(layer9, W10) + b10)\n",
    "\n",
    "    return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = -tf.reduce_mean(y_data * tf.log(hypothesis()) + (1 - y_data) * tf.log(1 - hypothesis()))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = tf.cast(hypothesis() > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, y_data), dtype = tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  0 cost =  0.71622705 [[ 1.0254498  -1.0860786   0.26425552  0.3823337   1.0479139 ]\n",
      " [-0.49248424 -0.9744123  -1.4663627   0.02379355  0.55209035]\n",
      " [-1.2831033   0.6591696   1.3149027  -0.37441415 -0.9767244 ]\n",
      " [-1.2465814  -0.3392333   0.24129823  1.0436403   0.07003616]\n",
      " [ 0.45342574 -0.6465525   0.39732793 -3.1424086  -0.14653848]]\n",
      "step =  100 cost =  0.40890318 [[ 1.009751   -1.0860786   0.30702835  0.38845417  1.0521133 ]\n",
      " [-0.49248424 -0.9744123  -1.4663627   0.02379355  0.55209035]\n",
      " [-1.2863944   0.6591696   1.3277533  -0.3747088  -0.9853517 ]\n",
      " [-1.2549876  -0.3392333   0.24129823  1.0523641   0.05683164]\n",
      " [ 0.43812063 -0.6465525   0.45315224 -3.1427925  -0.18968517]]\n",
      "step =  200 cost =  0.13243383 [[ 0.9944789  -1.0860786   0.30882335  0.39946932  1.0526706 ]\n",
      " [-0.49248424 -0.9744123  -1.4663627   0.02379355  0.55209035]\n",
      " [-1.2883245   0.6591696   1.3445041  -0.37486538 -0.9944677 ]\n",
      " [-1.2651166  -0.3392333   0.24129148  1.0623606   0.03755357]\n",
      " [ 0.42806962 -0.6465525   0.53094035 -3.1429543  -0.24592473]]\n",
      "step =  300 cost =  0.06292432 [[ 0.9849057  -1.0860786   0.32076433  0.40677053  1.0655875 ]\n",
      " [-0.49248424 -0.9744123  -1.4663627   0.02379355  0.55209035]\n",
      " [-1.2892431   0.6591696   1.3509424  -0.37487778 -0.9951232 ]\n",
      " [-1.2718344  -0.3392333   0.24128908  1.0681005   0.02481963]\n",
      " [ 0.42268127 -0.6465525   0.5638469  -3.1429687  -0.2590833 ]]\n",
      "step =  400 cost =  0.033672035 [[ 0.9832187  -1.0860786   0.3221753   0.4111535   1.0739985 ]\n",
      " [-0.49248424 -0.9744123  -1.4663627   0.02379355  0.55209035]\n",
      " [-1.28939     0.6591696   1.3537284  -0.37482625 -0.9955853 ]\n",
      " [-1.2753373  -0.3392333   0.24128862  1.0708289   0.01851868]\n",
      " [ 0.42136878 -0.6465525   0.57803375 -3.1429064  -0.26594913]]\n",
      "step =  500 cost =  0.022409042 [[ 0.9824265  -1.0860786   0.32635134  0.41450197  1.0770522 ]\n",
      " [-0.49248424 -0.9744123  -1.4663627   0.02379355  0.55209035]\n",
      " [-1.289438    0.6591696   1.3558624  -0.3747758  -0.9962267 ]\n",
      " [-1.2777842  -0.3392333   0.24128842  1.0727144   0.01439123]\n",
      " [ 0.4207787  -0.6465525   0.5882776  -3.142839   -0.27151087]]\n",
      "step =  600 cost =  0.016347848 [[ 0.981865   -1.0860786   0.32935125  0.4168555   1.0794109 ]\n",
      " [-0.49248424 -0.9744123  -1.4663627   0.02379355  0.55209035]\n",
      " [-1.2894771   0.6591696   1.3573709  -0.37475052 -0.9966779 ]\n",
      " [-1.2796874  -0.3392333   0.24128835  1.0741583   0.01129792]\n",
      " [ 0.4203289  -0.6465525   0.5952342  -3.1428049  -0.27519846]]\n",
      "step =  700 cost =  0.011091863 [[ 0.9838672  -1.0860786   0.3291397   0.41072965  1.0798248 ]\n",
      " [-0.49248424 -0.9744123  -1.4663627   0.02379355  0.55209035]\n",
      " [-1.2895321   0.6591696   1.3586849  -0.37512574 -0.9972167 ]\n",
      " [-1.281201   -0.3392333   0.24128824  1.0752833   0.00889342]\n",
      " [ 0.4192706  -0.6465525   0.6019175  -3.1433873  -0.27862513]]\n",
      "step =  800 cost =  0.008545423 [[ 0.984684   -1.0860786   0.3286047   0.4074575   1.079836  ]\n",
      " [-0.49248424 -0.9744123  -1.4663627   0.02379355  0.55209035]\n",
      " [-1.289651    0.6591696   1.3597169  -0.37531212 -0.99768084]\n",
      " [-1.2819304  -0.3392333   0.24128823  1.075791    0.00770632]\n",
      " [ 0.4185204  -0.6465525   0.605892   -3.1437118  -0.2805484 ]]\n",
      "step =  900 cost =  0.007303236 [[ 0.9853704  -1.0860786   0.32783306  0.4053068   1.0798209 ]\n",
      " [-0.49248424 -0.9744123  -1.4663627   0.02379355  0.55209035]\n",
      " [-1.289761    0.6591696   1.360695   -0.37543297 -0.998162  ]\n",
      " [-1.282417   -0.3392333   0.24128823  1.0761274   0.00691835]\n",
      " [ 0.41801396 -0.6465525   0.6090274  -3.143934   -0.28216013]]\n",
      "step =  1000 cost =  0.005891278 [[ 0.9856451  -1.0860786   0.3270158   0.403946    1.079571  ]\n",
      " [-0.49248424 -0.9744123  -1.4663627   0.02379355  0.55209035]\n",
      " [-1.2898693   0.6591696   1.3615764  -0.37551823 -0.9986073 ]\n",
      " [-1.2829329  -0.3392333   0.24128823  1.076495    0.00610827]\n",
      " [ 0.417599   -0.6465525   0.6115295  -3.1440966  -0.28347352]]\n",
      "\n",
      "Hypothesis:  [[0.0101131 ]\n",
      " [0.99910945]\n",
      " [0.99948126]\n",
      " [0.01191907]] \n",
      "Correct:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1001):\n",
    "        sess.run(train, feed_dict = {X: x_data, Y: y_data})\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(\"step = \", step, \"cost = \", sess.run(cost, feed_dict = {X: x_data, Y: y_data}),sess.run(W2))\n",
    "    h, c, a = sess.run([hypothesis(), predicted, accuracy], feed_dict = {X: x_data, Y: y_data})\n",
    "    print(\"\\nHypothesis: \", h, \"\\nCorrect: \", c, \"\\nAccuracy: \", a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
