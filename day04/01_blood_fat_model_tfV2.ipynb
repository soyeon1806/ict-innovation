{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "data_path = os.path.join(\"..\", \"data\", \"Blood_fat.csv\")\n",
    "data = np.loadtxt(data_path, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([x_row1[0] for x_row1 in data], dtype = np.float64)\n",
    "x2 = np.array([x_row2[1] for x_row2 in data], dtype = np.float64)\n",
    "y_data = np.array([y_row[2] for y_row in data], dtype = np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기 a와 y절편 b의 값을 임의로 정함\n",
    "# 단, 기울기의 범위는 0 ~ 10, y 절편은 0 ~ 100 사이에서 변하도록 함\n",
    "a1 = tf.Variable(tf.random.uniform([1], 0, 10, dtype = tf.float64, seed = 0))\n",
    "a2 = tf.Variable(tf.random.uniform([1], 0, 10, dtype = tf.float64, seed = 0))\n",
    "b = tf.Variable(tf.random.uniform([1], 0, 100, dtype = tf.float64, seed = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가설 함수\n",
    "def hypothesis(a1, a2, b):\n",
    "    return x1 * a1 + x2 * a2 + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수\n",
    "def cost(a1, a2, b):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(hypothesis(a1, a2, b) - y_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "opt = tf.keras.optimizers.SGD(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 579.8910245142281, [7.99948474], [7.7921178], [18.52939248]\n",
      "100 45.41474237804825, [1.81858573], [4.38207256], [18.44364284]\n",
      "200 43.996090811869976, [1.60566912], [4.61357998], [18.44613521]\n",
      "300 43.41569622961187, [1.48650464], [4.82167379], [18.44964724]\n",
      "400 43.06807733662892, [1.3943015], [4.98266316], [18.45327794]\n",
      "500 42.86254819635756, [1.32341485], [5.1064104], [18.45699489]\n",
      "600 42.74196474969345, [1.26912714], [5.20115769], [18.46077446]\n",
      "700 42.67152660449445, [1.22764613], [5.27353051], [18.46459961]\n",
      "800 42.63046825614693, [1.19599181], [5.32873526], [18.46845812]\n",
      "900 42.606547139625626, [1.17185323], [5.37080936], [18.47234118]\n",
      "1000 42.592596247294935, [1.15345204], [5.40285978], [18.47624238]\n",
      "1100 42.584437130621424, [1.13942584], [5.42726675], [18.48015704]\n",
      "1200 42.57963953739357, [1.1287336], [5.44584902], [18.48408174]\n",
      "1300 42.57679196219261, [1.12058107], [5.45999423], [18.4880139]\n",
      "1400 42.575075257392015, [1.11436278], [5.47076019], [18.49195164]\n",
      "1500 42.574014265606735, [1.10961742], [5.47895283], [18.49589353]\n",
      "1600 42.57333344095115, [1.10599361], [5.48518602], [18.4998385]\n",
      "1700 42.57287301862774, [1.10322379], [5.48992726], [18.50378575]\n",
      "1800 42.57254037374297, [1.10110417], [5.49353254], [18.50773466]\n",
      "1900 42.5722818092394, [1.09947962], [5.49627291], [18.51168477]\n",
      "2000 42.57206619655535, [1.098232], [5.49835476], [18.51563574]\n",
      "2100 42.571875490601556, [1.09727136], [5.49993523], [18.5195873]\n",
      "2200 42.57169923084836, [1.09652922], [5.50113397], [18.52353925]\n",
      "2300 42.571531353495565, [1.09595345], [5.50204207], [18.52749144]\n",
      "2400 42.571368343475015, [1.09550434], [5.50272889], [18.53144375]\n",
      "2500 42.57120816317714, [1.09515166], [5.50324723], [18.53539609]\n",
      "2600 42.57104963144676, [1.09487241], [5.5036373], [18.5393484]\n",
      "2700 42.57089206359696, [1.09464906], [5.50392971], [18.54330062]\n",
      "2800 42.57073506273196, [1.09446828], [5.50414777], [18.54725271]\n",
      "2900 42.57057839878136, [1.0943199], [5.50430921], [18.55120466]\n",
      "3000 42.5704219383791, [1.0941962], [5.50442755], [18.55515642]\n",
      "3100 42.57026560421594, [1.09409128], [5.50451308], [18.559108]\n",
      "3200 42.57010935147723, [1.09400067], [5.50457362], [18.56305936]\n",
      "3300 42.5699531541844, [1.09392094], [5.50461514], [18.56701051]\n",
      "3400 42.56979699727765, [1.09384951], [5.50464218], [18.57096144]\n",
      "3500 42.56964087202635, [1.0937844], [5.50465819], [18.57491213]\n",
      "3600 42.56948477336875, [1.09372409], [5.50466581], [18.5788626]\n",
      "3700 42.569328698369794, [1.09366745], [5.50466704], [18.58281282]\n",
      "3800 42.56917264532718, [1.09361359], [5.50466341], [18.5867628]\n",
      "3900 42.569016613253226, [1.09356186], [5.50465607], [18.59071255]\n",
      "4000 42.56886060157448, [1.09351174], [5.50464591], [18.59466204]\n",
      "4100 42.56870460995755, [1.09346286], [5.5046336], [18.5986113]\n",
      "4200 42.56854863820827, [1.09341491], [5.50461966], [18.60256031]\n",
      "4300 42.56839268621314, [1.09336768], [5.50460448], [18.60650907]\n",
      "4400 42.56823675390541, [1.09332099], [5.50458835], [18.61045759]\n",
      "4500 42.56808084124546, [1.09327473], [5.5045715], [18.61440585]\n",
      "4600 42.56792494820939, [1.09322877], [5.5045541], [18.61835388]\n",
      "4700 42.56776907478236, [1.09318306], [5.50453628], [18.62230165]\n",
      "4800 42.56761322095486, [1.09313754], [5.50451815], [18.62624918]\n",
      "4900 42.56745738672042, [1.09309216], [5.50449977], [18.63019646]\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for i in range(5000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost(a1, a2, b)\n",
    "    gradients = tape.gradient(loss, [a1, a2, b])\n",
    "    opt.apply_gradients(zip(gradients, [a1, a2, b]))\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(i, f'{loss.numpy()}, {a1. numpy()}, {a2. numpy()}, {b.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[363.65622479]\n",
      "[208.51618762]\n",
      "[375.91521382]\n",
      "[260.28185964]\n",
      "[415.46114154]\n",
      "[231.66640529]\n",
      "[241.62156548]\n",
      "[295.49484273]\n",
      "[418.74028355]\n",
      "[342.80983596]\n",
      "[180.25393577]\n",
      "[286.55424043]\n",
      "[375.91521382]\n",
      "[207.54087445]\n",
      "[413.39278103]\n",
      "[358.26947756]\n",
      "[271.36931189]\n",
      "[385.71339513]\n",
      "[375.83672438]\n",
      "[295.41635329]\n",
      "[336.33004139]\n",
      "[218.47134781]\n",
      "[315.2089395]\n",
      "[298.93096363]\n",
      "[252.63052829]\n"
     ]
    }
   ],
   "source": [
    "# 결과 출력\n",
    "da1 = a1.numpy()\n",
    "da2 = a2.numpy()\n",
    "db = b.numpy()\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    print((da1 * x1[i]) + (da2 * x2[i]) + db)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
